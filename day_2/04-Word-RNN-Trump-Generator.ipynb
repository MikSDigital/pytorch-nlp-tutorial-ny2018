{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\"\n",
    "IGNORE_INDEX_VALUE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTrumpTweets(object):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.data  \n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class TrumpTweetVectorizer(object):\n",
    "    def __init__(self, word_vocab, max_seq_length):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"word_vocab\": self.word_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"word_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"word_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, tweet_df):\n",
    "        vocab = Vocabulary(use_unks=False,\n",
    "                           use_start_end=True,\n",
    "                           use_mask=True,\n",
    "                           start_token=START_TOKEN,\n",
    "                           end_token=END_TOKEN)\n",
    "        max_seq_length = 0\n",
    "        for text in tweet_df.text:\n",
    "            split_text = text.split(\" \")\n",
    "            vocab.add_many(split_text)\n",
    "            if len(split_text) > max_seq_length:\n",
    "                max_seq_length = len(split_text)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "        return cls(vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, tweet_df, split='train'):\n",
    "        vectorizer = cls.fit(tweet_df)\n",
    "        return vectorizer, vectorizer.transform(tweet_df, split)\n",
    "\n",
    "    def transform(self, tweet_df, split='train'):\n",
    "        tweet_df = tweet_df[tweet_df.split==split].reset_index()\n",
    "        num_data = len(tweet_df)\n",
    "        \n",
    "        x_words = np.zeros((num_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_words = np.ones((num_data, self.max_seq_length), dtype=np.int64) * IGNORE_INDEX_VALUE\n",
    "\n",
    "        for index, row in tweet_df.iterrows():\n",
    "            converted = list(self.word_vocab.map(row.text.split(' '), include_start_end=True))\n",
    "            x_version = converted[:-1]\n",
    "            y_version = converted[1:]\n",
    "            \n",
    "            x_words[index, :len(x_version)] = x_version\n",
    "            y_words[index, :len(y_version)] = y_version\n",
    "            \n",
    "        return VectorizedTrumpTweets(x_words, y_words)\n",
    "\n",
    "# vec data\n",
    "\n",
    "\n",
    "class VectorizedTrumpTweets(Dataset):\n",
    "    def __init__(self, x_words, y_words):\n",
    "        self.x_words = x_words\n",
    "        self.y_words = y_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_words': self.x_words[index],\n",
    "                'y_words': self.y_words[index],\n",
    "                'x_lengths': len(self.x_words[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, use_cuda=False):\n",
    "        \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = Variable(tensor)\n",
    "            if use_cuda:\n",
    "                out_data_dict[name] = out_data_dict[name].cuda()\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_parameter(*size):\n",
    "    out = Parameter(torch.FloatTensor(*size))\n",
    "    torch.nn.init.xavier_normal(out)\n",
    "    return out\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index. \n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().data.cpu().numpy() - 1\n",
    "    # alternatively:\n",
    "    # out = []\n",
    "    # for batch_index, column_index in enumerate(x_lengths):\n",
    "    #     out.append(y_out[batch_index, column_index])\n",
    "    # return torch.stack(out)\n",
    "    return torch.stack([y_out[batch_index, column_index]\n",
    "                        for batch_index, column_index in enumerate(x_lengths)])\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = Variable(torch.zeros((batch_size, self.hidden_size)))\n",
    "        \n",
    "        if x_in.is_cuda:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "    \n",
    "    \n",
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, \n",
    "                 batch_first=True):\n",
    "        super(WordRNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size, \n",
    "                                padding_idx=0)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        \n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, \n",
    "                               hidden_size=hidden_size, \n",
    "                               batch_first=batch_first)\n",
    "    \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_in = self.emb(x_in)\n",
    "        y_out = self.rnn(x_in)\n",
    "\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def sequence_loss(net_output, y_true, loss_func=F.cross_entropy):\n",
    "    net_output, y_true = normalize_sizes(net_output, y_true)\n",
    "    return F.cross_entropy(net_output, y_true, ignore_index=IGNORE_INDEX_VALUE)\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.cpu().max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().data[0]\n",
    "    n_valid = valid_indices.sum().data[0]\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling functions\n",
    "\n",
    "in the last couple of examples, we've seen sampling.  we include the sampling code up front here so that we can sample during training to get a sense of what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(emb, rnn, fc, h_t=None, idx_t=None, n=20, temp=1):\n",
    "    hiddens = [h_t]\n",
    "    indices = [idx_t]\n",
    "    out_dists = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        x_t = emb(idx_t)\n",
    "        h_t = rnn._compute_next_hidden(x_t, h_t)\n",
    "        \n",
    "        y_t = fc(h_t)\n",
    "        y_t = F.softmax( y_t / temp, dim=1)\n",
    "        idx_t = torch.multinomial(y_t, 1)[:, 0]\n",
    "        \n",
    "        \n",
    "        hiddens.append(h_t)\n",
    "        indices.append(idx_t)\n",
    "        out_dists.append(y_t)\n",
    "     \n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def make_initial_hidden(batch_size, hidden_size):\n",
    "    out = Variable(torch.ones(batch_size, hidden_size))\n",
    "    return out\n",
    "\n",
    "def make_initial_x(batch_size, vectorizer):\n",
    "    out = Variable(torch.ones(batch_size) * vectorizer.word_vocab.start_index).long()\n",
    "    return out\n",
    "\n",
    "def decode_one(vectorizer, seq):\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if vectorizer.word_vocab.start_index == i:\n",
    "            continue\n",
    "        if vectorizer.word_vocab.end_index == i:\n",
    "            return ' '.join(out)\n",
    "        out.append(vectorizer.word_vocab.lookup(i))\n",
    "    return ' '.join(out)\n",
    "            \n",
    "def decode_matrix(vectorizer, mat):\n",
    "    mat = mat.cpu().data.numpy()\n",
    "    return [decode_one(vectorizer, mat[i]) for i in range(len(mat))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make, Train, and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not enabled\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    trump_csv=\"data/trump.csv\",\n",
    "    glove_filename=\"data/glove.6B.100d.txt\",\n",
    "    batch_size=128,\n",
    "    cuda=False,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    load_zoo_model=True,\n",
    "    zoo={\n",
    "        'filename': 'modelzoo/wordrnn_emb100_hid64_trump_tweets_predict.state',\n",
    "        'vocab': 'modelzoo/trump_twitter.vocab',\n",
    "        'comments': 'pre-trained trump sequence prediction (& generation)',\n",
    "        'parameters': {\n",
    "            'embedding_size': 100,\n",
    "            'hidden_size': 64\n",
    "        }\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "if args.cuda:\n",
    "    print(\"CUDA is enabled\")\n",
    "else:\n",
    "    print(\"CUDA is not enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: set this to false to learn from scratch!\n",
    "# args.load_zoo_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n"
     ]
    }
   ],
   "source": [
    "raw_data = RawTrumpTweets(args.trump_csv).get_data()\n",
    "\n",
    "if os.path.exists(args.zoo['vocab']):\n",
    "    vectorizer = TrumpTweetVectorizer.load(args.zoo['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = TrumpTweetVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "\n",
    "train_dataset = vectorizer.transform(raw_data, split='train')\n",
    "test_dataset = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "zoo_params = args.zoo['parameters']\n",
    "\n",
    "net = WordRNN(embedding_size=zoo_params['embedding_size'], \n",
    "              hidden_size=zoo_params['hidden_size'],\n",
    "              in_vocab_size=len(vectorizer.word_vocab), \n",
    "              out_vocab_size=len(vectorizer.word_vocab), \n",
    "              batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick inspection of model performance \n",
    "\n",
    "Before we load the model weights, what do the samples look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thru PACs Willie TAKE joint',\n",
       " 'TACKY 52 855 Celebration Policy',\n",
       " 'PENCE unauthorized trying copies burn',\n",
       " 'Recruitment approximately CommonCore state deplorables',\n",
       " 'am we arriving II guns',\n",
       " 'Wayne throughly Thank Course prefer',\n",
       " 'Mobile mr substance DOMINATING Drops',\n",
       " 'falsify Republicanpresidentialdebate CBS gratifying !!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial vectors\n",
    "initial_hidden = make_initial_hidden(batch_size=8, hidden_size=args.zoo['parameters']['hidden_size'])\n",
    "initial_x = make_initial_x(batch_size=8, vectorizer=vectorizer)\n",
    "\n",
    "# sampled matrix of indices\n",
    "sample_matrix = sample(net.emb, net.rnn, net.fc, \n",
    "                       initial_hidden, initial_x,\n",
    "                       temp=0.8, n=5)\n",
    "\n",
    "# decode matrix into text!\n",
    "decode_matrix(vectorizer, sample_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  loading model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state dict!\n"
     ]
    }
   ],
   "source": [
    "if args.load_zoo_model and os.path.exists(args.zoo['filename']):\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(args.zoo['filename'], map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introspection post model weight load\n",
    "\n",
    "notice we drop the `n` argument to sample this time. This was done assuming pretrained model weights were loaded.  Thish is because we assume the model is able to predict the end of sequence now! before, it could potentially go on forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poll is going to say .. for Trump , but I have been saying ! # Utah \" We',\n",
       " 'I got to my office ALL . Info :',\n",
       " 'Poll : Donald Trump \" Trump works Hotel and women and focus for the incredible for Trump and the people',\n",
       " '\" Clinton Dallas 1 / 15 - Breitbart & amp ; discuss :',\n",
       " 'I hope you coming out of the race . Refuse .\"',\n",
       " 'Poll yet she cant even started to run for her misconduct in . Bad system is stagnant back !',\n",
       " 'with you are on at 7 : 02 A . M . Enjoy !',\n",
       " 'I know polls , who has win for the great police in the polls , I will be there .']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial vectors\n",
    "initial_hidden = make_initial_hidden(batch_size=8, hidden_size=args.zoo['parameters']['hidden_size'])\n",
    "initial_x = make_initial_x(batch_size=8, vectorizer=vectorizer)\n",
    "\n",
    "# sampled matrix of indices\n",
    "sample_matrix = sample(net.emb, net.rnn, net.fc, \n",
    "                       initial_hidden, initial_x,\n",
    "                       temp=0.8)\n",
    "\n",
    "# decode matrix into text!\n",
    "decode_matrix(vectorizer, sample_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mode enabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d028de4e61e74426955081c8c70592d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f35ed17a094ed9abcef61792c4e219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd325842f0a4892bac81c6300895658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "if args.cuda:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()\n",
    "    \n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "# loss function\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "num_train_batches = len(train_dataset) // args.batch_size\n",
    "train_bar = tqdm_notebook(desc='training', total=num_train_batches, position=2)\n",
    "\n",
    "num_test_batches = len(test_dataset) // args.batch_size\n",
    "test_bar = tqdm_notebook(desc='test', total=num_test_batches, position=3)\n",
    "\n",
    "# history\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        batch_generator = generate_batches(train_dataset, batch_size=args.batch_size, use_cuda=args.cuda)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "        \n",
    "        net.train()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            \n",
    "            # step 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words']\n",
    "            \n",
    "            # step 3\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            \n",
    "            # step 4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # bonus steps: bookkeeping\n",
    "            \n",
    "            per_epoch_loss.append(loss.cpu().data[0])\n",
    "            \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            train_bar.update()\n",
    "            \n",
    "            train_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                  accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        train_loss_history.append(np.mean(per_epoch_loss))\n",
    "        train_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # loop over test dataset\n",
    "        \n",
    "        batch_generator = generate_batches(test_dataset, batch_size=args.batch_size, use_cuda=args.cuda)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "            \n",
    "        # set it to eval mode; this turns stochastic functions off\n",
    "        net.eval()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1: compute output\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words'] \n",
    "            \n",
    "            # step 2: compute metrics\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            per_epoch_loss.append(loss.cpu().data[0])\n",
    "          \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            test_bar.update()\n",
    "            \n",
    "            test_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                 accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        test_loss_history.append(np.mean(per_epoch_loss))\n",
    "        test_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # update bars\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss_history[-1], \n",
    "                              train_accuracy=train_accuracy_history[-1],\n",
    "                              test_loss=test_loss_history[-1],\n",
    "                              test_accuracy=test_accuracy_history[-1])\n",
    "        epoch_bar.update()\n",
    "        test_bar.n = 0\n",
    "        train_bar.n = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical Scenario\n",
    "\n",
    "Up until now, you haven't had pretrained weights. You've only been using the freshly initialized network. Suddenly, you realize pre-trained word vectors exist and can be used in your network!\n",
    "\n",
    "So, we will start by loading the glove word vectors and then incorporating them into our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def load_word_vectors(filename):\n",
    "    word_to_index = {}\n",
    "    word_vectors = []\n",
    "    \n",
    "    with open(filename) as fp:\n",
    "        for line in tqdm_notebook(fp.readlines(), leave=False):\n",
    "            line = line.split(\" \")\n",
    "            \n",
    "            word = line[0]\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            \n",
    "            vec = np.array([float(x) for x in line[1:]])\n",
    "            word_vectors.append(vec)\n",
    "    word_vector_size = len(word_vectors[0])\n",
    "    return word_to_index, word_vectors, word_vector_size\n",
    "\n",
    "word_to_index, word_vectors, word_vector_size = load_word_vectors(args.glove_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "now, we want to collate what we have from the word vectors with what is is on our vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10311, 100])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.emb.weight.size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mode enabled\n"
     ]
    }
   ],
   "source": [
    "net = WordRNN(embedding_size=zoo_params['embedding_size'], \n",
    "              hidden_size=zoo_params['hidden_size'],\n",
    "              in_vocab_size=len(vectorizer.word_vocab), \n",
    "              out_vocab_size=len(vectorizer.word_vocab), \n",
    "              batch_first=True)\n",
    "if args.cuda:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10310), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "9446 replaced\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for word, emb_index in tqdm_notebook(vectorizer.word_vocab.items(), leave=False):\n",
    "    if word.lower() in word_to_index:\n",
    "        n += 1\n",
    "        glove_index = word_to_index[word.lower()]\n",
    "        glove_vec = torch.FloatTensor(word_vectors[glove_index])\n",
    "        if net.emb.weight.is_cuda:\n",
    "            glove_vec = glove_vec.cuda()\n",
    "        net.emb.weight.data[emb_index, :].set_(glove_vec)\n",
    "\n",
    "print(n, 'replaced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-running training with these vectors\n",
    "\n",
    "While you won't be able to really appreciate the gains if training on a cpu, if you were to run the code again (this is the same training routine as above), you would see faster convergence as a lot of the ground work is already done by the pre-trained embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mode enabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750afc62b2404a0d94054f6b6007a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647e0f2d5fad46dc85c61cfb4a75f9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5956f88224f9403bb84f2c313dcd83bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "if args.cuda:\n",
    "    print(\"CUDA mode enabled\")\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print(\"CUDA mode not enabled\")\n",
    "    net = net.cpu()\n",
    "    \n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "# loss function\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "num_train_batches = len(train_dataset) // args.batch_size\n",
    "train_bar = tqdm_notebook(desc='training', total=num_train_batches, position=2)\n",
    "\n",
    "num_test_batches = len(test_dataset) // args.batch_size\n",
    "test_bar = tqdm_notebook(desc='test', total=num_test_batches, position=3)\n",
    "\n",
    "# history\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        batch_generator = generate_batches(train_dataset, batch_size=args.batch_size, use_cuda=args.cuda)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "        \n",
    "        net.train()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            \n",
    "            # step 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # step 2\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words']\n",
    "            \n",
    "            # step 3\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            \n",
    "            # step 4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # bonus steps: bookkeeping\n",
    "            \n",
    "            per_epoch_loss.append(loss.cpu().data[0])\n",
    "            \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            train_bar.update()\n",
    "            \n",
    "            train_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                  accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        train_loss_history.append(np.mean(per_epoch_loss))\n",
    "        train_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # loop over test dataset\n",
    "        \n",
    "        batch_generator = generate_batches(test_dataset, batch_size=args.batch_size, use_cuda=args.cuda)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "            \n",
    "        # set it to eval mode; this turns stochastic functions off\n",
    "        net.eval()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1: compute output\n",
    "            y_pred = net(batch_dict['x_words'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_words'] \n",
    "            \n",
    "            # step 2: compute metrics\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            per_epoch_loss.append(loss.cpu().data[0])\n",
    "          \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_words'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            test_bar.update()\n",
    "            \n",
    "            test_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                 accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        test_loss_history.append(np.mean(per_epoch_loss))\n",
    "        test_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # update bars\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss_history[-1], \n",
    "                              train_accuracy=train_accuracy_history[-1],\n",
    "                              test_loss=test_loss_history[-1],\n",
    "                              test_accuracy=test_accuracy_history[-1])\n",
    "        epoch_bar.update()\n",
    "        test_bar.n = 0\n",
    "        train_bar.n = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I support begging for ME , Donald , whod watch 12 .',\n",
       " 'whose interview in Florida ! # Trump2016 # 2A \" We Thanks law enforcement officers . Thank you for the',\n",
       " 'who spoke about concerning it to run against \" , Trump is not a news conference - dummy !',\n",
       " 'who voted for one of the rigged system under budget that Secret Service never spoke to create it very well',\n",
       " 'who voted for an absolutely ? is doing \"',\n",
       " 'Poll numbers was a great day . Focus severely place out of Illinois ! # TrumpPence16 # BigLeagueTruth # DemDebate',\n",
       " 'I love very much . A real class is going on ?',\n",
       " 'who voted for the presidency , and MSM is a great SILENT MAJORITY looming !',\n",
       " 'I know it is 100 --- given !',\n",
       " 'Poll , he just announced in USA ! Do make video plans to tell me to take the RNC and',\n",
       " 'I will be interviewed by on at 8 : 30 A . M . So nice from Citi , 000',\n",
       " 'I know that Trump gives his speech in history ? They burned us safe CNN - # VoteTrump today !\"',\n",
       " 'I know you are the worst New York ! Crooked Hillary would have been front page on .',\n",
       " 'for the Conservative !!\" of The Womens British Open will be HUUUGE .\"',\n",
       " 'I love you were great on tonight . He is what you wanna agree !',\n",
       " 'The Dallas , North Carolina . I am elected President , will be back soon !',\n",
       " 'Poll debate \" is the only one who got fired tired of the media !',\n",
       " 'Poll country is the man who dont have to shut down . It was a good thing . # Trump2016',\n",
       " 'I know that speaks you when he said with me . We raised , military , then many other candidates',\n",
       " 'Poll is in an interviewer help ( biggest failed ads against Hillary Clinton .',\n",
       " 'Thursday Obama / this week on # TrumpPence16 -- hope !',\n",
       " 'about Crooked Hillary Clinton is the only for President of New Hampshire on the main stage . TRUMP 2016 \"',\n",
       " 'Poll is in a disaster . Going to advisers by with a somewhat V . P . P . FBI',\n",
       " 'Vets and appreciate your potatoes - common core - Visa \" Trump Continues vets voters destroying USA ! # VoteTrump2016',\n",
       " 'Does anyone else . Mark Levin debate better even got a terrible job .',\n",
       " 'Poll debate poll - thank you ! # MAGA',\n",
       " 'today in Florida , Iowa . I will continue to LONG , Christianity .',\n",
       " 'today , who lost me for a very disloyal chain !',\n",
       " 'in NJ , is a soft wimpy questions ? He holds gets easy !',\n",
       " 'with you are doing the only candidate that I am honored to has about me is terrible . We can',\n",
       " 'I have President Obama as that I am millions of VOTES there cant stand up the biased and phony ...',\n",
       " 'Poll is in the teens numbers night in . Also , we are superior and all of your statements !',\n",
       " 'Poll , we dont think Crooked Hillary Clinton is the only one who the people are making America . Lets',\n",
       " 'by the GOP field focused on secret tape that he will be respect the President .',\n",
       " 'Poll , Trump , Crooked Hillary can her pick , has bad judgement !',\n",
       " 'Poll , debate was right . Really tough and angry from that only writes report - Breitbart \"',\n",
       " 'by the roast of politics , Mr . Trump is the man who was a great success for the people',\n",
       " 'Poll is in many states ! Get out & amp ; VOTE last nights away from the tonight !',\n",
       " 'Poll is going to have a great guy !',\n",
       " 'if you know that is now great again !',\n",
       " 'I love the democrats debate . Their anybody highly recommend the debate and that many lies , she was so',\n",
       " 'in .... polls are looking for a big rally ( I am going to win !',\n",
       " 'the economy is a lifelong democrat . And Bernie Sanders , jealous failures have at his disloyalty .',\n",
       " 'Poll is on real country and MAKE AMERICA GREAT AGAIN !',\n",
       " 'with you are saying your amazing people !',\n",
       " 'Poll , only one who has not like a massive million --- not 4 Trump ( wrong ) # MakeAmericaGreatAgain',\n",
       " 'and clearly was called a Prez determining winner !',\n",
       " 'Poll : Trump Leads !',\n",
       " 'who can mark if I started to me . Against steelworkers and miners . Husband signed NAFTA and why knows',\n",
       " 'for Trump , not raise in working life by a total disaster - and energy on taxes ratings is on',\n",
       " 'I will be on at 7 : 02 - Trump \"',\n",
       " 'Wow , who is not totally dishonest !',\n",
       " '\" special to show the liberal nomination ? No other GOP , Fighter and phony - Rubio & amp ;',\n",
       " 'of the great coach , Bobby Knight , with two days , Time ! The U . S . S',\n",
       " 'Everybody is using a speech that works was - why the U . S . Rory on hard for __',\n",
       " 'who can fix this true !',\n",
       " 'I were highly overrated & amp ; Putin has done off , open from somebody -- Albert Einstein',\n",
       " 'the agenda , that I am the only one who is self funding !',\n",
       " 'I know that is the only network failing has about the leaders and Bush got caught on illegal immigration ,',\n",
       " 'Poll is going to make it possible !',\n",
       " 'who voted for a total winner !',\n",
       " 'Poll , are a big percentage of the speech ), Washington D . C . riots !',\n",
       " 'for Clinton flunky after they knew it !',\n",
       " 'I will be interviewed on at 8 : 00 P . M . and then America up . MASTERMINDS /',\n",
       " 'Poll , Bush did Hillary is drawing to be missed by saying !',\n",
       " 'The interview and the people who know of the Deal and other drugs , within they sent up on their',\n",
       " 'who has seen that Ted to get African American youth \"',\n",
       " 'and how small every country are commenting votes . SAD !',\n",
       " 'Poll is in race in every poll - cant she allowed to respond ? They noticed alright ? # MakeAmericaGreatAgain',\n",
       " 'by the Republican Party of my RALLIES , Nevada on # MakeAmericaGreatAgain hat is now available online . To shop',\n",
       " 'who spoke about the best will be back at \" & amp ; non - we are going to WIN',\n",
       " 'who can live streaming !',\n",
       " 'Poll , doesn ۪ t get the other regulation , he should on me .',\n",
       " 'Poll poll has made anchor and Wall Street money on and enthusiastic worse than very nice !',\n",
       " 'Headline , I will easily Trump ۪ t blame video !',\n",
       " 'who voted for a great way to speeches this mess !',\n",
       " 'I know that Crooked Hillary Clinton is not total disaster . No into the NJ Boxing Hall of Sarah Root',\n",
       " 'Poll - great and law enforcement to audit and is now ! # Trump2016',\n",
       " 'who were very mostly that American will be an exciting woman and replace . We will register us Donald will',\n",
       " 'Trump is the failed State and criminals that I said \" NO \" Media 6 on 11 / money on',\n",
       " 'The V . P . M . on & amp ; safety and me . Not now . Grab the',\n",
       " 'New Iowa Poll / # SNL # tcot \"',\n",
       " 'Border Walker and the Democrats former fan of our current administration , tune in Iowa , so much in your',\n",
       " 'Clinton desperately said for Trump . # GOPDebate \"',\n",
       " 'who voted for their pick .',\n",
       " 'I will be on tonight at 7pm . Both said with the legendary news , who said waste of politics',\n",
       " 'who can candidate ! # VoteTrump \" Rupert Daytona Medicaid , 000 , 000 , 000 , a ad Trump',\n",
       " '100 percent Clinton 40 % Via Cant Find Money Prez Times video',\n",
       " 'Poll , are the president of a win ( 2 ) at 7pm ET Donald Trump ! # MakeAmericaGreatAgain Tickets',\n",
       " 'who should have been very right . Make America Great Again !',\n",
       " 'who got to Trump University ! No crime campaign - I am sending out the first time as they do',\n",
       " 'Democrats keep on television - thank you .',\n",
       " 'Show were thugs who isis much stand , Michigan ! # LEO # tellingitlikeitis \"',\n",
       " 'Poll new book , Christi Berglund pity .\"',\n",
       " '..... Ahead of African - Wall Street loser ! Thank you to tell the media is saying something !\"',\n",
       " 'Poll , Bush , no match for years , Trump beats # VPDebate # ImWithYou # FITN',\n",
       " 'Poll is in a loser , and totally serve at 8 : 00pm . Hes this but Jeb by ISIS',\n",
       " 'Poll debate - and has happened to WIN together , we will MAKE AMERICA GREAT AGAIN !',\n",
       " 'Poll , didnt win on the first ballot he is only one has no clue & amp ; Sanders moved',\n",
       " 'I think her husband and close and very presidential !']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = net.cpu()\n",
    "\n",
    "# initial vectors\n",
    "initial_hidden = make_initial_hidden(batch_size=8, hidden_size=args.zoo['parameters']['hidden_size'])\n",
    "initial_x = make_initial_x(batch_size=8, vectorizer=vectorizer)\n",
    "\n",
    "# sampled matrix of indices\n",
    "sample_matrix = sample(net.emb, net.rnn, net.fc, \n",
    "                       initial_hidden, initial_x,\n",
    "                       temp=0.8)\n",
    "\n",
    "# decode matrix into text!\n",
    "decode_matrix(vectorizer, sample_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
